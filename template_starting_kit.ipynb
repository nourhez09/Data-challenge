{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <a href=\"https://www.dataia.eu/\">\n",
    "        <img border=\"0\" src=\"https://github.com/ramp-kits/template-kit/raw/main/img/DATAIA-h.png\" width=\"90%\"></a>\n",
    "</div>\n",
    "\n",
    "# Template Kit for RAMP challenge\n",
    "\n",
    "<i> Thomas Moreau (Inria) </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Describe the challenge, in particular:\n",
    "\n",
    "- Where the data comes from?\n",
    "- What is the task this challenge aims to solve?\n",
    "- Why does it matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "The goal of this section is to show what's in the data, and how to play with it.\n",
    "This is the first set in any data science project, and here, you should give a sense of the data the participants will be working with.\n",
    "\n",
    "You can first load and describe the data, and then show some interesting properties of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\clip\\clip.py:57: UserWarning: C:\\Users\\Akshita Kumar/.cache/clip\\ViT-B-32.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "100%|███████████████████████████████████████| 338M/338M [03:30<00:00, 1.68MiB/s]\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "\n",
    "import problem\n",
    "X, y = problem.get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = problem.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensors\n",
    "import torch\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 175/175 [00:11<00:00, 15.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from submissions.starting_kit import CondImageGenerator\n",
    "model = CondImageGenerator.ConditionalVAE(input_channels=3, hidden_dim_enc=128, hidden_dim_dec=128,\n",
    "                 latent_dim=128, n_layers_enc=4, n_layers_dec=4,\n",
    "                 condition_dim=768, image_size=128, cond_new_dim=768, device='cuda')\n",
    "model.fit(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 34.09it/s]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mramp_custom\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclip_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPScore\n\u001b[32m      2\u001b[39m score_type = CLIPScore()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m score = \u001b[43mscore_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Akshita Kumar\\Desktop\\Data-challenge\\ramp_custom\\clip_score.py:58\u001b[39m, in \u001b[36mCLIPScore.__call__\u001b[39m\u001b[34m(self, texts, images)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03mCompute the CLIP-based score.\u001b[39;00m\n\u001b[32m     40\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[33;03m    Inverse of the mean cosine similarity (lower = better).\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 1) Preprocess and encode the text\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Tokenize text prompts\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m text_tokens = \u001b[43mclip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     60\u001b[39m     text_features = \u001b[38;5;28mself\u001b[39m.model.encode_text(text_tokens)  \u001b[38;5;66;03m# (n_samples, 512 or 768, depending on model)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\clip\\clip.py:230\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(texts, context_length, truncate)\u001b[39m\n\u001b[32m    228\u001b[39m sot_token = _tokenizer.encoder[\u001b[33m\"\u001b[39m\u001b[33m<|startoftext|>\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    229\u001b[39m eot_token = _tokenizer.encoder[\u001b[33m\"\u001b[39m\u001b[33m<|endoftext|>\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m all_tokens = \u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43msot_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43meot_token\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(torch.__version__) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m1.8.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    232\u001b[39m     result = torch.zeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype=torch.long)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\clip\\clip.py:230\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    228\u001b[39m sot_token = _tokenizer.encoder[\u001b[33m\"\u001b[39m\u001b[33m<|startoftext|>\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    229\u001b[39m eot_token = _tokenizer.encoder[\u001b[33m\"\u001b[39m\u001b[33m<|endoftext|>\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m all_tokens = [[sot_token] + \u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m + [eot_token] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(torch.__version__) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m1.8.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    232\u001b[39m     result = torch.zeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype=torch.long)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\clip\\simple_tokenizer.py:123\u001b[39m, in \u001b[36mSimpleTokenizer.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[32m    122\u001b[39m     bpe_tokens = []\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     text = whitespace_clean(\u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m).lower()\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m    125\u001b[39m         token = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.byte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\clip\\simple_tokenizer.py:51\u001b[39m, in \u001b[36mbasic_clean\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbasic_clean\u001b[39m(text):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     text = \u001b[43mftfy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfix_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     text = html.unescape(html.unescape(text))\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\ftfy\\__init__.py:349\u001b[39m, in \u001b[36mfix_text\u001b[39m\u001b[34m(text, config, **kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m pos = \u001b[32m0\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m pos < \u001b[38;5;28mlen\u001b[39m(text):\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     textbreak = \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, pos) + \u001b[32m1\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m textbreak == \u001b[32m0\u001b[39m:\n\u001b[32m    351\u001b[39m         textbreak = \u001b[38;5;28mlen\u001b[39m(text)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "from ramp_custom.clip_score import CLIPScore\n",
    "score_type = CLIPScore()\n",
    "score = score_type(X_test, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Akshita Kumar\\capstone\\Scripts\\ramp-test.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\cli\\testing.py\", line 117, in start\n",
      "    main()\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\click\\core.py\", line 1161, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\click\\core.py\", line 1082, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\click\\core.py\", line 1443, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\click\\core.py\", line 788, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\cli\\testing.py\", line 102, in main\n",
      "    assert_submission(ramp_kit_dir=ramp_kit_dir,\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\testing.py\", line 103, in assert_submission\n",
      "    problem = assert_read_problem(ramp_kit_dir)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\testing.py\", line 33, in assert_read_problem\n",
      "    return import_module_from_source(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\importing.py\", line 36, in import_module_from_source\n",
      "    spec.loader.exec_module(module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\Akshita Kumar\\Desktop\\Data-challenge\\.\\problem.py\", line 10, in <module>\n",
      "    from ramp_custom.workflow_ConditionedImageGenerator import ConditionedImageGenerator\n",
      "ModuleNotFoundError: No module named 'ramp_custom'\n"
     ]
    }
   ],
   "source": [
    "!ramp-test --submission submissions/starting_kit --quick-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge evaluation\n",
    "\n",
    "A particularly important point in a challenge is to describe how it is evaluated. This is the section where you should describe the metric that will be used to evaluate the participants' submissions, as well as your evaluation strategy, in particular if there is some complexity in the way the data should be split to ensure valid results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from rampwf.score_types.base import BaseScoreType\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class CLIPScore(BaseScoreType):\n",
    "    \"\"\"\n",
    "    CLIP-like Score for text-conditioned image generation.\n",
    "\n",
    "    This metric:\n",
    "      1. Accepts pre-computed text embeddings (shape: (n_samples, 768)).\n",
    "      2. Computes image embeddings from generated images (numpy array of shape (n_samples, H, W, C))\n",
    "         using a ResNet-18 backbone.\n",
    "      3. Projects the ResNet features (512-dim) to 768 dimensions so they match the text embeddings.\n",
    "      4. Normalizes both embeddings and computes the cosine similarity.\n",
    "      5. Returns the inverse of the mean cosine similarity (so that lower scores are better).\n",
    "    \"\"\"\n",
    "    is_lower_the_better = True\n",
    "    minimum = 0.0\n",
    "    maximum = float('inf')\n",
    "    \n",
    "    def __init__(self, name='clip score', precision=2):\n",
    "        self.name = name\n",
    "        self.precision = precision\n",
    "\n",
    "        # 1) Initialize a ResNet-18 backbone and remove its final classification layer.\n",
    "        self.image_model = resnet18(pretrained=True)\n",
    "        self.image_model.fc = nn.Identity()  # Now outputs a 512-dim feature vector\n",
    "\n",
    "        # 2) Define a linear projection from 512 -> 768 to match text embeddings.\n",
    "        self.projection = nn.Linear(512, 768, bias=False)\n",
    "\n",
    "        # Set models to evaluation mode and freeze parameters.\n",
    "        self.image_model.eval()\n",
    "        for param in self.image_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.projection.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # # 3) Define a transform that normalizes images as expected by ResNet.\n",
    "        # self.transform = T.Compose([\n",
    "        #     T.ConvertImageDtype(torch.float),  # Convert image to float (0,1)\n",
    "        #     T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                 std=[0.229, 0.224, 0.225]),\n",
    "        # ])\n",
    "\n",
    "    def __call__(self, text_embeddings: np.ndarray, generated_images: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute a CLIP-like score between text embeddings and generated images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_embeddings : np.ndarray\n",
    "            Pre-computed text embeddings. Expected shape: (n_samples, 768).\n",
    "        generated_images : np.ndarray\n",
    "            Generated images. Expected shape: (n_samples, height, width, channels) with 3 channels (RGB).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The inverse of the mean cosine similarity between text and image embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # 1) Preprocess images and convert to Torch tensors.\n",
    "        # -------------------------------------------------------\n",
    "        images_tensor = torch.from_numpy(generated_images)\n",
    "        images_tensor = images_tensor.float() / 255.0  # Scale pixel values from [0,255] to [0,1]\n",
    "        #images_tensor = self.transform(images_tensor)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # 2) Extract features with the ResNet-18 backbone.\n",
    "        # -------------------------------------------------------\n",
    "        with torch.no_grad():\n",
    "            features = self.image_model(images_tensor)  # (n_samples, 512)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # 3) Project features to match the text embedding dimension (768).\n",
    "        # -------------------------------------------------------\n",
    "        with torch.no_grad():\n",
    "            image_embeddings_torch = self.projection(features)  # (n_samples, 768)\n",
    "\n",
    "        # Convert the image embeddings to a NumPy array.\n",
    "        image_embeddings = image_embeddings_torch.cpu().numpy()\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # 4) Normalize embeddings and compute cosine similarity.\n",
    "        # -------------------------------------------------------\n",
    "        text_norm = text_embeddings / (norm(text_embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "        if isinstance(text_norm, torch.Tensor):\n",
    "            text_norm = text_norm.numpy()\n",
    "        image_norm = image_embeddings / (norm(image_embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "        cos_sim = np.abs(np.sum(text_norm * image_norm, axis=1))  # Cosine similarity for each sample\n",
    "        mean_cos_sim = np.mean(cos_sim)\n",
    "\n",
    "        # -------------------------------------------------------\n",
    "        # 5) Return the inverse of the mean similarity (lower is better).\n",
    "        # -------------------------------------------------------\n",
    "        clip_score = 1.0 / (mean_cos_sim + 1e-8)\n",
    "        return clip_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.90282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshita Kumar\\AppData\\Local\\Temp\\ipykernel_27352\\2747156422.py:93: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  text_norm = text_embeddings / (norm(text_embeddings, axis=1, keepdims=True) + 1e-8)\n"
     ]
    }
   ],
   "source": [
    "score_type = CLIPScore()\n",
    "score = score_type(X_test, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from rampwf.score_types.base import BaseScoreType\n",
    "\n",
    "class FID(BaseScoreType):\n",
    "    \"\"\"\n",
    "    Fréchet Inception Distance (FID) for image generation.\n",
    "    \n",
    "    This metric computes the difference between the statistics of the\n",
    "    generated images and the real images. Lower values indicate that the\n",
    "    generated images are closer to the real ones.\n",
    "    \n",
    "    Note: This implementation flattens images and computes covariances\n",
    "    directly. For more robust evaluations, it is common to extract features\n",
    "    using a pre-trained network such as Inception.\n",
    "    \"\"\"\n",
    "    is_lower_the_better = True\n",
    "    minimum = 0.0\n",
    "    maximum = float('inf')\n",
    "    \n",
    "    def __init__(self, name='FID', precision=2):\n",
    "        self.name = name\n",
    "        self.precision = precision\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the FID between ground truth images and generated images.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : numpy array\n",
    "            Ground truth images. Shape: (n_samples, height, width, channels).\n",
    "        y_pred : numpy array\n",
    "            Generated images. Shape: (n_samples, height, width, channels).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The computed FID score.\n",
    "        \"\"\"\n",
    "        # Flatten images to vectors\n",
    "        y_true_flat = y_true.mean(axis=(1,2))\n",
    "        y_pred_flat = y_pred.mean(axis=(1,2))\n",
    "        \n",
    "        # Compute mean vectors\n",
    "        mu_true = np.mean(y_true_flat, axis=0)\n",
    "        mu_pred = np.mean(y_pred_flat, axis=0)\n",
    "        \n",
    "        # Compute covariance matrices\n",
    "        sigma_true = np.cov(y_true_flat, rowvar=False)\n",
    "        sigma_pred = np.cov(y_pred_flat, rowvar=False)\n",
    "        \n",
    "        # Compute squared difference between means\n",
    "        diff = mu_true - mu_pred\n",
    "        diff_squared = np.sum(diff**2)\n",
    "        \n",
    "        # Compute the square root of the product of covariance matrices\n",
    "        covmean = sqrtm(sigma_true.dot(sigma_pred))\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        \n",
    "        fid = diff_squared + np.trace(sigma_true + sigma_pred - 2 * covmean)\n",
    "        return fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.190257105446081)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FID_score = FID()\n",
    "FID_score(y_test.numpy(), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5255, 0.5333, 0.5373,  ..., 0.6471, 0.6431, 0.6353],\n",
       "         [0.5333, 0.5412, 0.5451,  ..., 0.6510, 0.6471, 0.6353],\n",
       "         [0.5412, 0.5490, 0.5529,  ..., 0.6510, 0.6510, 0.6392],\n",
       "         ...,\n",
       "         [0.1961, 0.2275, 0.2745,  ..., 0.1647, 0.1529, 0.1529],\n",
       "         [0.2314, 0.2588, 0.3020,  ..., 0.1647, 0.1529, 0.1490],\n",
       "         [0.2471, 0.2706, 0.3059,  ..., 0.1647, 0.1569, 0.1608]],\n",
       "\n",
       "        [[0.3608, 0.3647, 0.3725,  ..., 0.5216, 0.5255, 0.5216],\n",
       "         [0.3647, 0.3686, 0.3725,  ..., 0.5255, 0.5255, 0.5216],\n",
       "         [0.3686, 0.3725, 0.3765,  ..., 0.5294, 0.5294, 0.5294],\n",
       "         ...,\n",
       "         [0.0471, 0.0745, 0.1176,  ..., 0.0588, 0.0510, 0.0549],\n",
       "         [0.0784, 0.1098, 0.1412,  ..., 0.0588, 0.0510, 0.0510],\n",
       "         [0.0941, 0.1176, 0.1451,  ..., 0.0588, 0.0588, 0.0627]],\n",
       "\n",
       "        [[0.2118, 0.2196, 0.2235,  ..., 0.4549, 0.4549, 0.4510],\n",
       "         [0.2196, 0.2235, 0.2275,  ..., 0.4549, 0.4549, 0.4510],\n",
       "         [0.2235, 0.2275, 0.2353,  ..., 0.4510, 0.4510, 0.4471],\n",
       "         ...,\n",
       "         [0.0000, 0.0157, 0.0431,  ..., 0.0157, 0.0235, 0.0353],\n",
       "         [0.0157, 0.0392, 0.0627,  ..., 0.0157, 0.0235, 0.0314],\n",
       "         [0.0275, 0.0471, 0.0627,  ..., 0.0157, 0.0314, 0.0431]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission format\n",
    "\n",
    "Here, you should describe the submission format. This is the format the participants should follow to submit their predictions on the RAMP plateform.\n",
    "\n",
    "This section also show how to use the `ramp-workflow` library to test the submission locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline workflow\n",
    "\n",
    "The input data are stored in a dataframe. To go from a dataframe to a numpy array we will use a scikit-learn column transformer. The first example we will write will just consist in selecting a subset of columns we want to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "To submit your code, you can refer to the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
