{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <a href=\"https://www.dataia.eu/\">\n",
    "        <img border=\"0\" src=\"https://github.com/ramp-kits/template-kit/raw/main/img/DATAIA-h.png\" width=\"90%\"></a>\n",
    "</div>\n",
    "\n",
    "# Template Kit for RAMP challenge\n",
    "\n",
    "<i> Thomas Moreau (Inria) </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Describe the challenge, in particular:\n",
    "\n",
    "- Where the data comes from?\n",
    "- What is the task this challenge aims to solve?\n",
    "- Why does it matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "The goal of this section is to show what's in the data, and how to play with it.\n",
    "This is the first set in any data science project, and here, you should give a sense of the data the participants will be working with.\n",
    "\n",
    "You can first load and describe the data, and then show some interesting properties of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "\n",
    "import problem\n",
    "X, y = problem.get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = problem.get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensors\n",
    "import torch\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 175/175 [00:10<00:00, 16.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from submissions.starting_kit import CondImageGenerator\n",
    "model = CondImageGenerator.ConditionalVAE(input_channels=3, hidden_dim_enc=128, hidden_dim_dec=128,\n",
    "                 latent_dim=128, n_layers_enc=4, n_layers_dec=4,\n",
    "                 condition_dim=768, image_size=128, cond_new_dim=768, device='cuda')\n",
    "model.fit(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 63.04it/s]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284.28366\n"
     ]
    }
   ],
   "source": [
    "from ramp_custom.clip_score import CLIPScore\n",
    "score_type = CLIPScore()\n",
    "score = score_type(X_test, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Akshita Kumar\\capstone\\Scripts\\ramp-test.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\cli\\testing.py\", line 117, in start\n",
      "    main()\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\click\\core.py\", line 1161, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\click\\core.py\", line 1082, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\click\\core.py\", line 1443, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\click\\core.py\", line 788, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\cli\\testing.py\", line 102, in main\n",
      "    assert_submission(ramp_kit_dir=ramp_kit_dir,\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\testing.py\", line 103, in assert_submission\n",
      "    problem = assert_read_problem(ramp_kit_dir)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\testing.py\", line 33, in assert_read_problem\n",
      "    return import_module_from_source(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Akshita Kumar\\capstone\\Lib\\site-packages\\rampwf\\utils\\importing.py\", line 36, in import_module_from_source\n",
      "    spec.loader.exec_module(module)\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\Akshita Kumar\\Desktop\\Data-challenge\\.\\problem.py\", line 10, in <module>\n",
      "    from ramp_custom.workflow_ConditionedImageGenerator import ConditionedImageGenerator\n",
      "ModuleNotFoundError: No module named 'ramp_custom'\n"
     ]
    }
   ],
   "source": [
    "!ramp-test --submission submissions/starting_kit --quick-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge evaluation\n",
    "\n",
    "A particularly important point in a challenge is to describe how it is evaluated. This is the section where you should describe the metric that will be used to evaluate the participants' submissions, as well as your evaluation strategy, in particular if there is some complexity in the way the data should be split to ensure valid results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'clip' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model, preprocess = \u001b[43mclip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mViT-B/32\u001b[39m\u001b[33m\"\u001b[39m, device=device)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Example image preprocessing\u001b[39;00m\n\u001b[32m     11\u001b[39m image_tensor = preprocess(your_image).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'clip' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from rampwf.score_types.base import BaseScoreType\n",
    "\n",
    "class CLIPScore(BaseScoreType):\n",
    "    \"\"\"\n",
    "    CLIP Score for text-conditioned image generation.\n",
    "    \n",
    "    This metric computes the cosine similarity between the image and text embeddings.\n",
    "    A higher similarity indicates a better match.\n",
    "    \"\"\"\n",
    "    is_lower_the_better = False  # Higher scores are better\n",
    "    minimum = -1.0  # Cosine similarity ranges from -1 to 1\n",
    "    maximum = 1.0\n",
    "    \n",
    "    def __init__(self, name='clip score', precision=2):\n",
    "        self.name = name\n",
    "        self.precision = precision\n",
    "\n",
    "    def __call__(self, text_embeddings, image_embeddings):\n",
    "        \"\"\"\n",
    "        Compute the CLIP score between text embeddings and image embeddings.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text_embeddings : numpy array\n",
    "            Text embeddings extracted from CLIP. Shape: (n_samples, embedding_dim).\n",
    "        image_embeddings : numpy array\n",
    "            Image embeddings extracted from CLIP. Shape: (n_samples, embedding_dim).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The averaged CLIP score (cosine similarity between text and image embeddings).\n",
    "        \"\"\"\n",
    "        #if text_embeddings.shape != image_embeddings.shape:\n",
    "         #   raise ValueError(\"Text and image embeddings must have the same shape.\")\n",
    "         # Ensure text and image embeddings have the same shape\n",
    "        n_samples, channels, height, width = image_embeddings.shape\n",
    "        image_embeddings_flat = image_embeddings.reshape(n_samples, -1)  # Flatten to (n_samples, channels * height * width)\n",
    "\n",
    "        # Normalize both embeddings to unit vectors\n",
    "        text_norm = text_embeddings / (norm(text_embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "        text_norm = text_norm.numpy()\n",
    "        image_norm = image_embeddings_flat / (norm(image_embeddings_flat, axis=1, keepdims=True) + 1e-8)\n",
    "        image_norm = image_norm\n",
    "\n",
    "        # Compute pairwise cosine similarity\n",
    "        cos_sim = np.sum(text_norm * image_norm, axis=1)\n",
    "\n",
    "        # Average over all samples\n",
    "        clip_score = np.mean(cos_sim)\n",
    "        print(f\"CLIP score: {clip_score:.{self.precision}f}\")\n",
    "        return clip_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 33.23it/s]\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshita Kumar\\AppData\\Local\\Temp\\ipykernel_5692\\1947010917.py:43: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  text_norm = text_embeddings / (norm(text_embeddings, axis=1, keepdims=True) + 1e-8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1655,768) (1655,49152) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m score_type = CLIPScore()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m score = \u001b[43mscore_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mCLIPScore.__call__\u001b[39m\u001b[34m(self, text_embeddings, image_embeddings)\u001b[39m\n\u001b[32m     46\u001b[39m image_norm = image_norm\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Compute pairwise cosine similarity\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m cos_sim = np.sum(\u001b[43mtext_norm\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_norm\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Average over all samples\u001b[39;00m\n\u001b[32m     52\u001b[39m clip_score = np.mean(cos_sim)\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (1655,768) (1655,49152) "
     ]
    }
   ],
   "source": [
    "score_type = CLIPScore()\n",
    "score = score_type(X_test, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1655, 3, 128, 128)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1655, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshita Kumar\\AppData\\Local\\Temp\\ipykernel_5692\\1680937394.py:40: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  text_norm = text_embeddings / (norm(text_embeddings, axis=1, keepdims=True) + 1e-8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1655,768) (1655,3,128,128) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m score_type = CLIPScore()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m score = \u001b[43mscore_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mCLIPScore.__call__\u001b[39m\u001b[34m(self, text_embeddings, image_embeddings)\u001b[39m\n\u001b[32m     41\u001b[39m image_norm = image_embeddings / (norm(image_embeddings, axis=\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[32m1e-8\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Compute pairwise cosine similarity\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m cos_sim = np.sum(\u001b[43mtext_norm\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_norm\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Average over all samples\u001b[39;00m\n\u001b[32m     47\u001b[39m clip_score = np.mean(cos_sim)\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (1655,768) (1655,3,128,128) "
     ]
    }
   ],
   "source": [
    "score_type = CLIPScore()\n",
    "score = score_type(X_test, result)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from rampwf.score_types.base import BaseScoreType\n",
    "\n",
    "class FID(BaseScoreType):\n",
    "    \"\"\"\n",
    "    Fréchet Inception Distance (FID) for image generation.\n",
    "    \n",
    "    This metric computes the difference between the statistics of the\n",
    "    generated images and the real images. Lower values indicate that the\n",
    "    generated images are closer to the real ones.\n",
    "    \n",
    "    Note: This implementation flattens images and computes covariances\n",
    "    directly. For more robust evaluations, it is common to extract features\n",
    "    using a pre-trained network such as Inception.\n",
    "    \"\"\"\n",
    "    is_lower_the_better = True\n",
    "    minimum = 0.0\n",
    "    maximum = float('inf')\n",
    "    \n",
    "    def __init__(self, name='FID', precision=2):\n",
    "        self.name = name\n",
    "        self.precision = precision\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the FID between ground truth images and generated images.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : numpy array\n",
    "            Ground truth images. Shape: (n_samples, height, width, channels).\n",
    "        y_pred : numpy array\n",
    "            Generated images. Shape: (n_samples, height, width, channels).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The computed FID score.\n",
    "        \"\"\"\n",
    "        # Flatten images to vectors\n",
    "        y_true_flat = y_true.reshape(y_true.shape[0], -1)\n",
    "        y_pred_flat = y_pred.reshape(y_pred.shape[0], -1)\n",
    "        \n",
    "        # Compute mean vectors\n",
    "        mu_true = np.mean(y_true_flat, axis=0)\n",
    "        mu_pred = np.mean(y_pred_flat, axis=0)\n",
    "        \n",
    "        # Compute covariance matrices\n",
    "        sigma_true = np.cov(y_true_flat, rowvar=False)\n",
    "        sigma_pred = np.cov(y_pred_flat, rowvar=False)\n",
    "        \n",
    "        # Compute squared difference between means\n",
    "        diff = mu_true - mu_pred\n",
    "        diff_squared = np.sum(diff**2)\n",
    "        \n",
    "        # Compute the square root of the product of covariance matrices\n",
    "        covmean = sqrtm(sigma_true.dot(sigma_pred))\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        \n",
    "        fid = diff_squared + np.trace(sigma_true + sigma_pred - 2 * covmean)\n",
    "        return fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1655, 3, 128, 128)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FID_score = FID()\n",
    "FID_score(y_test.numpy(), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission format\n",
    "\n",
    "Here, you should describe the submission format. This is the format the participants should follow to submit their predictions on the RAMP plateform.\n",
    "\n",
    "This section also show how to use the `ramp-workflow` library to test the submission locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline workflow\n",
    "\n",
    "The input data are stored in a dataframe. To go from a dataframe to a numpy array we will use a scikit-learn column transformer. The first example we will write will just consist in selecting a subset of columns we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/starting_kit/estimator.py\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def get_estimator():\n",
    "    pipe = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression()\n",
    "    )\n",
    "\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using a scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97222222 0.96527778 0.97212544 0.95121951 0.96167247]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(get_estimator(), X_df, y, cv=5, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "To submit your code, you can refer to the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
